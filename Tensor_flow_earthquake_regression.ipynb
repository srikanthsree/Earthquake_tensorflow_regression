{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('earthquake_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23412 entries, 0 to 23411\n",
      "Data columns (total 18 columns):\n",
      "Date                 23412 non-null object\n",
      "Time                 23412 non-null object\n",
      "Latitude             23412 non-null float64\n",
      "Longitude            23412 non-null float64\n",
      "Type                 23412 non-null object\n",
      "Depth                23412 non-null float64\n",
      "Magnitude            23412 non-null float64\n",
      "NormalizedLat        23412 non-null float64\n",
      "NormalizedLong       23412 non-null float64\n",
      "NormalizedDepth      23412 non-null float64\n",
      "NormalizedMag        23412 non-null float64\n",
      "Unnamed: 11          0 non-null float64\n",
      "Unnamed: 12          0 non-null float64\n",
      "Unnamed: 13          2 non-null object\n",
      "Latitude_max_min     2 non-null float64\n",
      "Longitude_max_min    2 non-null float64\n",
      "Depth_max_min        2 non-null float64\n",
      "Magnitude_max_min    2 non-null float64\n",
      "dtypes: float64(14), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Type</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>NormalizedLat</th>\n",
       "      <th>NormalizedLong</th>\n",
       "      <th>NormalizedDepth</th>\n",
       "      <th>NormalizedMag</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Latitude_max_min</th>\n",
       "      <th>Longitude_max_min</th>\n",
       "      <th>Depth_max_min</th>\n",
       "      <th>Magnitude_max_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23744.00</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>19.246</td>\n",
       "      <td>145.616</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>131.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.590649</td>\n",
       "      <td>0.904493</td>\n",
       "      <td>0.189274</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>min</td>\n",
       "      <td>-77.080</td>\n",
       "      <td>-179.997</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23746.00</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>1.863</td>\n",
       "      <td>127.352</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>80.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.484060</td>\n",
       "      <td>0.853759</td>\n",
       "      <td>0.115675</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>max</td>\n",
       "      <td>86.005</td>\n",
       "      <td>179.998</td>\n",
       "      <td>700.0</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23747.00</td>\n",
       "      <td>0.7541</td>\n",
       "      <td>-20.579</td>\n",
       "      <td>-173.972</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.346451</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.030096</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23750.00</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>-59.076</td>\n",
       "      <td>-23.557</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.110396</td>\n",
       "      <td>0.434562</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23751.00</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>11.938</td>\n",
       "      <td>126.427</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.545838</td>\n",
       "      <td>0.851190</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Time  Latitude  Longitude        Type  Depth  Magnitude  \\\n",
       "0  23744.00  0.5724    19.246    145.616  Earthquake  131.6        6.0   \n",
       "1  23746.00  0.4790     1.863    127.352  Earthquake   80.0        5.8   \n",
       "2  23747.00  0.7541   -20.579   -173.972  Earthquake   20.0        6.2   \n",
       "3  23750.00  0.7845   -59.076    -23.557  Earthquake   15.0        5.8   \n",
       "4  23751.00  0.5645    11.938    126.427  Earthquake   15.0        5.8   \n",
       "\n",
       "   NormalizedLat  NormalizedLong  NormalizedDepth  NormalizedMag  Unnamed: 11  \\\n",
       "0       0.590649        0.904493         0.189274       0.138889          NaN   \n",
       "1       0.484060        0.853759         0.115675       0.083333          NaN   \n",
       "2       0.346451        0.016736         0.030096       0.194444          NaN   \n",
       "3       0.110396        0.434562         0.022964       0.083333          NaN   \n",
       "4       0.545838        0.851190         0.022964       0.083333          NaN   \n",
       "\n",
       "   Unnamed: 12 Unnamed: 13  Latitude_max_min  Longitude_max_min  \\\n",
       "0          NaN         min           -77.080           -179.997   \n",
       "1          NaN         max            86.005            179.998   \n",
       "2          NaN         NaN               NaN                NaN   \n",
       "3          NaN         NaN               NaN                NaN   \n",
       "4          NaN         NaN               NaN                NaN   \n",
       "\n",
       "   Depth_max_min  Magnitude_max_min  \n",
       "0           -1.1                5.5  \n",
       "1          700.0                9.1  \n",
       "2            NaN                NaN  \n",
       "3            NaN                NaN  \n",
       "4            NaN                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Earthquake', 'Nuclear Explosion', 'Explosion', 'Rock Burst'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(df[['Latitude','Longitude','Depth']])\n",
    "y = np.array(df[['Magnitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_norm = (x-np.amin(x,0))/(np.amax(x,0)-np.amin(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = len(x_norm)\n",
    "np.random.seed(0)\n",
    "num_features = x_norm.shape[1]\n",
    "num_output = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split vectors into train / eval sets\n",
    "index = np.arange(num_samples)\n",
    "np.random.shuffle(index)\n",
    "train_end = int(0.7* num_samples)\n",
    "validate_end = int(0.2 * num_samples) + train_end\n",
    "trainX, trainY = x_norm[index[:train_end]], y[index[:train_end]]\n",
    "validX, validY = x_norm[index[train_end:validate_end]], y[index[train_end:validate_end]]\n",
    "testX, testY = x_norm[index[validate_end:]], y[index[validate_end:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 20\n",
    "n_nodes_hl2 = 20\n",
    "\n",
    "batch_size = 100 # batches of features feed to network\n",
    "\n",
    "x = tf.placeholder('float', [None, num_features])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([num_features, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, num_output])),\n",
    "                    'biases':tf.Variable(tf.random_normal([num_output])),}\n",
    "\n",
    " \t# (input_data * weights) + biases\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    output = tf.matmul(l2,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Intial Training loss: [160.13422]\n",
      "Epoch: 1 Training loss: [107.91811] Validation loss: [106.63245]\n",
      "Epoch: 2 Training loss: [78.160492] Validation loss: [78.064529]\n",
      "Epoch: 3 Training loss: [67.54615] Validation loss: [68.28743]\n",
      "Epoch: 4 Training loss: [68.360725] Validation loss: [69.597618]\n",
      "Epoch: 5 Training loss: [72.051285] Validation loss: [73.519363]\n",
      "Epoch: 6 Training loss: [73.049614] Validation loss: [74.570259]\n",
      "Epoch: 7 Training loss: [69.781288] Validation loss: [71.248695]\n",
      "Epoch: 8 Training loss: [63.098755] Validation loss: [64.451538]\n",
      "Epoch: 9 Training loss: [54.743946] Validation loss: [55.934326]\n",
      "Epoch: 10 Training loss: [46.42004] Validation loss: [47.418095]\n",
      "Epoch: 11 Training loss: [39.493938] Validation loss: [40.272404]\n",
      "Epoch: 12 Training loss: [34.637306] Validation loss: [35.194172]\n",
      "Epoch: 13 Training loss: [31.777563] Validation loss: [32.130737]\n",
      "Epoch: 14 Training loss: [30.308392] Validation loss: [30.482786]\n",
      "Epoch: 15 Training loss: [29.357605] Validation loss: [29.397743]\n",
      "Epoch: 16 Training loss: [28.12055] Validation loss: [28.078281]\n",
      "Epoch: 17 Training loss: [26.134357] Validation loss: [26.064774]\n",
      "Epoch: 18 Training loss: [23.333502] Validation loss: [23.281715]\n",
      "Epoch: 19 Training loss: [19.993521] Validation loss: [19.991795]\n",
      "Epoch: 20 Training loss: [16.67803] Validation loss: [16.73177]\n",
      "Epoch: 21 Training loss: [13.677794] Validation loss: [13.784782]\n",
      "Epoch: 22 Training loss: [11.172005] Validation loss: [11.332928]\n",
      "Epoch: 23 Training loss: [9.2869844] Validation loss: [9.4888048]\n",
      "Epoch: 24 Training loss: [7.9729424] Validation loss: [8.1990004]\n",
      "Epoch: 25 Training loss: [7.0428371] Validation loss: [7.276752]\n",
      "Epoch: 26 Training loss: [6.281425] Validation loss: [6.5114427]\n",
      "Epoch: 27 Training loss: [5.5565991] Validation loss: [5.7777224]\n",
      "Epoch: 28 Training loss: [4.8185554] Validation loss: [5.0263381]\n",
      "Epoch: 29 Training loss: [4.0815463] Validation loss: [4.269485]\n",
      "Epoch: 30 Training loss: [3.4009354] Validation loss: [3.5607922]\n",
      "Epoch: 31 Training loss: [2.845417] Validation loss: [2.9753308]\n",
      "Epoch: 32 Training loss: [2.451591] Validation loss: [2.5521398]\n",
      "Epoch: 33 Training loss: [2.200356] Validation loss: [2.2748139]\n",
      "Epoch: 34 Training loss: [2.0287204] Validation loss: [2.0814309]\n",
      "Epoch: 35 Training loss: [1.8717078] Validation loss: [1.9096127]\n",
      "Epoch: 36 Training loss: [1.6747689] Validation loss: [1.7055501]\n",
      "Epoch: 37 Training loss: [1.4288906] Validation loss: [1.4584899]\n",
      "Epoch: 38 Training loss: [1.1682695] Validation loss: [1.1996881]\n",
      "Epoch: 39 Training loss: [0.94548029] Validation loss: [0.98066854]\n",
      "Epoch: 40 Training loss: [0.80113751] Validation loss: [0.8406176]\n",
      "Epoch: 41 Training loss: [0.74468744] Validation loss: [0.78755671]\n",
      "Epoch: 42 Training loss: [0.75091547] Validation loss: [0.79451549]\n",
      "Epoch: 43 Training loss: [0.77702296] Validation loss: [0.81931448]\n",
      "Epoch: 44 Training loss: [0.78594047] Validation loss: [0.8258515]\n",
      "Epoch: 45 Training loss: [0.76379216] Validation loss: [0.80021179]\n",
      "Epoch: 46 Training loss: [0.7213524] Validation loss: [0.7534306]\n",
      "Epoch: 47 Training loss: [0.68310189] Validation loss: [0.71084237]\n",
      "Epoch: 48 Training loss: [0.67008108] Validation loss: [0.69361621]\n",
      "Epoch: 49 Training loss: [0.68657225] Validation loss: [0.70633948]\n",
      "Epoch: 50 Training loss: [0.71910119] Validation loss: [0.735825]\n",
      "Epoch: 51 Training loss: [0.74628717] Validation loss: [0.7609477]\n",
      "Epoch: 52 Training loss: [0.75254285] Validation loss: [0.76615721]\n",
      "Epoch: 53 Training loss: [0.73636544] Validation loss: [0.74981314]\n",
      "Epoch: 54 Training loss: [0.70906818] Validation loss: [0.72289073]\n",
      "Epoch: 55 Training loss: [0.68576694] Validation loss: [0.70016587]\n",
      "Epoch: 56 Training loss: [0.67557114] Validation loss: [0.69045007]\n",
      "Epoch: 57 Training loss: [0.67694408] Validation loss: [0.69210607]\n",
      "Epoch: 58 Training loss: [0.68057984] Validation loss: [0.6958425]\n",
      "Epoch: 59 Training loss: [0.67668915] Validation loss: [0.69189811]\n",
      "Epoch: 60 Training loss: [0.66148239] Validation loss: [0.67644554]\n",
      "Epoch: 61 Training loss: [0.63874876] Validation loss: [0.65325469]\n",
      "Epoch: 62 Training loss: [0.61631536] Validation loss: [0.63037997]\n",
      "Epoch: 63 Training loss: [0.60045379] Validation loss: [0.61410147]\n",
      "Epoch: 64 Training loss: [0.5921393] Validation loss: [0.60535145]\n",
      "Epoch: 65 Training loss: [0.58722645] Validation loss: [0.60012907]\n",
      "Epoch: 66 Training loss: [0.5801686] Validation loss: [0.59311736]\n",
      "Epoch: 67 Training loss: [0.56797183] Validation loss: [0.58142519]\n",
      "Epoch: 68 Training loss: [0.55189067] Validation loss: [0.56623191]\n",
      "Epoch: 69 Training loss: [0.53594512] Validation loss: [0.55144465]\n",
      "Epoch: 70 Training loss: [0.52368993] Validation loss: [0.54047549]\n",
      "Epoch: 71 Training loss: [0.5159201] Validation loss: [0.53400862]\n",
      "Epoch: 72 Training loss: [0.5105415] Validation loss: [0.52981913]\n",
      "Epoch: 73 Training loss: [0.50465024] Validation loss: [0.52495766]\n",
      "Epoch: 74 Training loss: [0.49673384] Validation loss: [0.51779199]\n",
      "Epoch: 75 Training loss: [0.48760235] Validation loss: [0.5091027]\n",
      "Epoch: 76 Training loss: [0.47933885] Validation loss: [0.50109601]\n",
      "Epoch: 77 Training loss: [0.47362822] Validation loss: [0.4954834]\n",
      "Epoch: 78 Training loss: [0.47053826] Validation loss: [0.49247631]\n",
      "Epoch: 79 Training loss: [0.4686313] Validation loss: [0.49077043]\n",
      "Epoch: 80 Training loss: [0.46630713] Validation loss: [0.48883674]\n",
      "Epoch: 81 Training loss: [0.46291542] Validation loss: [0.48605594]\n",
      "Epoch: 82 Training loss: [0.45904616] Validation loss: [0.48295969]\n",
      "Epoch: 83 Training loss: [0.45581067] Validation loss: [0.48054636]\n",
      "Epoch: 84 Training loss: [0.45382929] Validation loss: [0.47932142]\n",
      "Epoch: 85 Training loss: [0.45276138] Validation loss: [0.4788518]\n",
      "Epoch: 86 Training loss: [0.45168018] Validation loss: [0.47812787]\n",
      "Epoch: 87 Training loss: [0.44984832] Validation loss: [0.4763816]\n",
      "Epoch: 88 Training loss: [0.44722497] Validation loss: [0.47358495]\n",
      "Epoch: 89 Training loss: [0.44434893] Validation loss: [0.47032747]\n",
      "Epoch: 90 Training loss: [0.44180781] Validation loss: [0.46729073]\n",
      "Epoch: 91 Training loss: [0.43976372] Validation loss: [0.4647271]\n",
      "Epoch: 92 Training loss: [0.43788952] Validation loss: [0.46239287]\n",
      "Epoch: 93 Training loss: [0.43573618] Validation loss: [0.45987949]\n",
      "Epoch: 94 Training loss: [0.43312648] Validation loss: [0.45701212]\n",
      "Epoch: 95 Training loss: [0.43026158] Validation loss: [0.45396712]\n",
      "Epoch: 96 Training loss: [0.42750785] Validation loss: [0.45107472]\n",
      "Epoch: 97 Training loss: [0.42509168] Validation loss: [0.44850519]\n",
      "Epoch: 98 Training loss: [0.42294213] Validation loss: [0.44615442]\n",
      "Epoch: 99 Training loss: [0.42081192] Validation loss: [0.44374222]\n",
      "Epoch: 100 Training loss: [0.41851825] Validation loss: [0.44109228]\n",
      "Epoch: 101 Training loss: [0.4160994] Validation loss: [0.43824086]\n",
      "Epoch: 102 Training loss: [0.41373417] Validation loss: [0.43540955]\n",
      "Epoch: 103 Training loss: [0.41158333] Validation loss: [0.43278998]\n",
      "Epoch: 104 Training loss: [0.40964502] Validation loss: [0.43041283]\n",
      "Epoch: 105 Training loss: [0.4077906] Validation loss: [0.42817822]\n",
      "Epoch: 106 Training loss: [0.4058921] Validation loss: [0.42596862]\n",
      "Epoch: 107 Training loss: [0.40392739] Validation loss: [0.42376086]\n",
      "Epoch: 108 Training loss: [0.4019731] Validation loss: [0.42161554]\n",
      "Epoch: 109 Training loss: [0.40011799] Validation loss: [0.41960135]\n",
      "Epoch: 110 Training loss: [0.39838123] Validation loss: [0.41770986]\n",
      "Epoch: 111 Training loss: [0.39670008] Validation loss: [0.41585517]\n",
      "Epoch: 112 Training loss: [0.39499512] Validation loss: [0.41395062]\n",
      "Epoch: 113 Training loss: [0.39324033] Validation loss: [0.41196981]\n",
      "Epoch: 114 Training loss: [0.39147463] Validation loss: [0.40996355]\n",
      "Epoch: 115 Training loss: [0.38975126] Validation loss: [0.40800285]\n",
      "Epoch: 116 Training loss: [0.38808692] Validation loss: [0.40612176]\n",
      "Epoch: 117 Training loss: [0.38645425] Validation loss: [0.40430406]\n",
      "Epoch: 118 Training loss: [0.38480943] Validation loss: [0.4025144]\n",
      "Epoch: 119 Training loss: [0.38313827] Validation loss: [0.40073004]\n",
      "Epoch: 120 Training loss: [0.38145992] Validation loss: [0.39896005]\n",
      "Epoch: 121 Training loss: [0.3798081] Validation loss: [0.39723703]\n",
      "Epoch: 122 Training loss: [0.37819493] Validation loss: [0.3955591]\n",
      "Epoch: 123 Training loss: [0.37660828] Validation loss: [0.39390767]\n",
      "Epoch: 124 Training loss: [0.37502247] Validation loss: [0.39225084]\n",
      "Epoch: 125 Training loss: [0.37343296] Validation loss: [0.3905746]\n",
      "Epoch: 126 Training loss: [0.37185043] Validation loss: [0.38889539]\n",
      "Epoch: 127 Training loss: [0.37029257] Validation loss: [0.38725245]\n",
      "Epoch: 128 Training loss: [0.36876845] Validation loss: [0.38564539]\n",
      "Epoch: 129 Training loss: [0.36726686] Validation loss: [0.38407135]\n",
      "Epoch: 130 Training loss: [0.36577174] Validation loss: [0.38251868]\n",
      "Epoch: 131 Training loss: [0.36427897] Validation loss: [0.38097885]\n",
      "Epoch: 132 Training loss: [0.36279708] Validation loss: [0.37945274]\n",
      "Epoch: 133 Training loss: [0.3613359] Validation loss: [0.37794691]\n",
      "Epoch: 134 Training loss: [0.35989383] Validation loss: [0.3764537]\n",
      "Epoch: 135 Training loss: [0.35846847] Validation loss: [0.37496218]\n",
      "Epoch: 136 Training loss: [0.35705081] Validation loss: [0.37346193]\n",
      "Epoch: 137 Training loss: [0.35563937] Validation loss: [0.37195602]\n",
      "Epoch: 138 Training loss: [0.35423616] Validation loss: [0.3704446]\n",
      "Epoch: 139 Training loss: [0.35284683] Validation loss: [0.36894017]\n",
      "Epoch: 140 Training loss: [0.35147223] Validation loss: [0.36744729]\n",
      "Epoch: 141 Training loss: [0.35010722] Validation loss: [0.36596641]\n",
      "Epoch: 142 Training loss: [0.34874931] Validation loss: [0.36449811]\n",
      "Epoch: 143 Training loss: [0.34739718] Validation loss: [0.36303824]\n",
      "Epoch: 144 Training loss: [0.34605274] Validation loss: [0.36158925]\n",
      "Epoch: 145 Training loss: [0.34472108] Validation loss: [0.36014965]\n",
      "Epoch: 146 Training loss: [0.34339878] Validation loss: [0.35871831]\n",
      "Epoch: 147 Training loss: [0.34208649] Validation loss: [0.35729325]\n",
      "Epoch: 148 Training loss: [0.34078419] Validation loss: [0.35587335]\n",
      "Epoch: 149 Training loss: [0.33949056] Validation loss: [0.35445988]\n",
      "Epoch: 150 Training loss: [0.33820724] Validation loss: [0.35305518]\n",
      "Epoch: 151 Training loss: [0.33693391] Validation loss: [0.35166386]\n",
      "Epoch: 152 Training loss: [0.33567309] Validation loss: [0.35028896]\n",
      "Epoch: 153 Training loss: [0.33442092] Validation loss: [0.34893072]\n",
      "Epoch: 154 Training loss: [0.3331798] Validation loss: [0.34758604]\n",
      "Epoch: 155 Training loss: [0.33194932] Validation loss: [0.34625623]\n",
      "Epoch: 156 Training loss: [0.33072707] Validation loss: [0.34493777]\n",
      "Epoch: 157 Training loss: [0.32951581] Validation loss: [0.34362817]\n",
      "Epoch: 158 Training loss: [0.32831478] Validation loss: [0.34232584]\n",
      "Epoch: 159 Training loss: [0.32712197] Validation loss: [0.34103265]\n",
      "Epoch: 160 Training loss: [0.32593659] Validation loss: [0.33974883]\n",
      "Epoch: 161 Training loss: [0.3247599] Validation loss: [0.3384749]\n",
      "Epoch: 162 Training loss: [0.32358962] Validation loss: [0.33721155]\n",
      "Epoch: 163 Training loss: [0.32242885] Validation loss: [0.33596012]\n",
      "Epoch: 164 Training loss: [0.32127535] Validation loss: [0.33471996]\n",
      "Epoch: 165 Training loss: [0.32012942] Validation loss: [0.33348629]\n",
      "Epoch: 166 Training loss: [0.31899095] Validation loss: [0.33226162]\n",
      "Epoch: 167 Training loss: [0.31785959] Validation loss: [0.33104762]\n",
      "Epoch: 168 Training loss: [0.31673768] Validation loss: [0.32984313]\n",
      "Epoch: 169 Training loss: [0.31562459] Validation loss: [0.32864332]\n",
      "Epoch: 170 Training loss: [0.31451878] Validation loss: [0.32744706]\n",
      "Epoch: 171 Training loss: [0.31342134] Validation loss: [0.32625481]\n",
      "Epoch: 172 Training loss: [0.31233263] Validation loss: [0.32506871]\n",
      "Epoch: 173 Training loss: [0.31125125] Validation loss: [0.32389405]\n",
      "Epoch: 174 Training loss: [0.31017649] Validation loss: [0.32272846]\n",
      "Epoch: 175 Training loss: [0.30910975] Validation loss: [0.32157186]\n",
      "Epoch: 176 Training loss: [0.30804986] Validation loss: [0.32042617]\n",
      "Epoch: 177 Training loss: [0.30699852] Validation loss: [0.3192901]\n",
      "Epoch: 178 Training loss: [0.30595541] Validation loss: [0.31816193]\n",
      "Epoch: 179 Training loss: [0.30492049] Validation loss: [0.31704265]\n",
      "Epoch: 180 Training loss: [0.30389205] Validation loss: [0.31593126]\n",
      "Epoch: 181 Training loss: [0.30287099] Validation loss: [0.31482667]\n",
      "Epoch: 182 Training loss: [0.30185825] Validation loss: [0.31373027]\n",
      "Epoch: 183 Training loss: [0.30085424] Validation loss: [0.31264228]\n",
      "Epoch: 184 Training loss: [0.29985833] Validation loss: [0.31155851]\n",
      "Epoch: 185 Training loss: [0.29887155] Validation loss: [0.31048301]\n",
      "Epoch: 186 Training loss: [0.29789123] Validation loss: [0.30941695]\n",
      "Epoch: 187 Training loss: [0.2969192] Validation loss: [0.30835918]\n",
      "Epoch: 188 Training loss: [0.29595602] Validation loss: [0.30730733]\n",
      "Epoch: 189 Training loss: [0.2950007] Validation loss: [0.30626348]\n",
      "Epoch: 190 Training loss: [0.2940546] Validation loss: [0.30523211]\n",
      "Epoch: 191 Training loss: [0.29311821] Validation loss: [0.30420852]\n",
      "Epoch: 192 Training loss: [0.29218924] Validation loss: [0.30319464]\n",
      "Epoch: 193 Training loss: [0.29126745] Validation loss: [0.30219302]\n",
      "Epoch: 194 Training loss: [0.29035357] Validation loss: [0.30120116]\n",
      "Epoch: 195 Training loss: [0.28944716] Validation loss: [0.30021766]\n",
      "Epoch: 196 Training loss: [0.28854758] Validation loss: [0.29924166]\n",
      "Epoch: 197 Training loss: [0.28765452] Validation loss: [0.29827467]\n",
      "Epoch: 198 Training loss: [0.28676787] Validation loss: [0.29731363]\n",
      "Epoch: 199 Training loss: [0.28588951] Validation loss: [0.29636005]\n",
      "Epoch: 200 Training loss: [0.28501889] Validation loss: [0.29541484]\n",
      "Epoch: 201 Training loss: [0.2841562] Validation loss: [0.29447722]\n",
      "Epoch: 202 Training loss: [0.28330004] Validation loss: [0.29354689]\n",
      "Epoch: 203 Training loss: [0.28245059] Validation loss: [0.29262343]\n",
      "Epoch: 204 Training loss: [0.28160825] Validation loss: [0.29170626]\n",
      "Epoch: 205 Training loss: [0.2807731] Validation loss: [0.29079682]\n",
      "Epoch: 206 Training loss: [0.27994442] Validation loss: [0.28989559]\n",
      "Epoch: 207 Training loss: [0.27912429] Validation loss: [0.28900197]\n",
      "Epoch: 208 Training loss: [0.27831089] Validation loss: [0.28811601]\n",
      "Epoch: 209 Training loss: [0.27750561] Validation loss: [0.28723824]\n",
      "Epoch: 210 Training loss: [0.27670848] Validation loss: [0.28636831]\n",
      "Epoch: 211 Training loss: [0.27591899] Validation loss: [0.28550434]\n",
      "Epoch: 212 Training loss: [0.27513659] Validation loss: [0.28464729]\n",
      "Epoch: 213 Training loss: [0.27436194] Validation loss: [0.28379819]\n",
      "Epoch: 214 Training loss: [0.2735939] Validation loss: [0.28295612]\n",
      "Epoch: 215 Training loss: [0.27283263] Validation loss: [0.28212109]\n",
      "Epoch: 216 Training loss: [0.27207682] Validation loss: [0.2812928]\n",
      "Epoch: 217 Training loss: [0.27132851] Validation loss: [0.28047228]\n",
      "Epoch: 218 Training loss: [0.27058643] Validation loss: [0.27965823]\n",
      "Epoch: 219 Training loss: [0.26985151] Validation loss: [0.27885073]\n",
      "Epoch: 220 Training loss: [0.26912242] Validation loss: [0.2780467]\n",
      "Epoch: 221 Training loss: [0.26839873] Validation loss: [0.27724868]\n",
      "Epoch: 222 Training loss: [0.26768067] Validation loss: [0.27645507]\n",
      "Epoch: 223 Training loss: [0.26696923] Validation loss: [0.27566919]\n",
      "Epoch: 224 Training loss: [0.26626304] Validation loss: [0.27488956]\n",
      "Epoch: 225 Training loss: [0.26556292] Validation loss: [0.27411661]\n",
      "Epoch: 226 Training loss: [0.2648682] Validation loss: [0.27334905]\n",
      "Epoch: 227 Training loss: [0.26417968] Validation loss: [0.2725873]\n",
      "Epoch: 228 Training loss: [0.26349667] Validation loss: [0.27183306]\n",
      "Epoch: 229 Training loss: [0.2628195] Validation loss: [0.27108458]\n",
      "Epoch: 230 Training loss: [0.26214749] Validation loss: [0.27034265]\n",
      "Epoch: 231 Training loss: [0.26148051] Validation loss: [0.26960668]\n",
      "Epoch: 232 Training loss: [0.26081926] Validation loss: [0.26887619]\n",
      "Epoch: 233 Training loss: [0.26016369] Validation loss: [0.26815128]\n",
      "Epoch: 234 Training loss: [0.25951362] Validation loss: [0.26743215]\n",
      "Epoch: 235 Training loss: [0.25886813] Validation loss: [0.26671782]\n",
      "Epoch: 236 Training loss: [0.25822827] Validation loss: [0.2660076]\n",
      "Epoch: 237 Training loss: [0.25759429] Validation loss: [0.26530185]\n",
      "Epoch: 238 Training loss: [0.25696626] Validation loss: [0.26460159]\n",
      "Epoch: 239 Training loss: [0.25634408] Validation loss: [0.26390821]\n",
      "Epoch: 240 Training loss: [0.25572664] Validation loss: [0.26321989]\n",
      "Epoch: 241 Training loss: [0.25511509] Validation loss: [0.26253793]\n",
      "Epoch: 242 Training loss: [0.25450721] Validation loss: [0.26186213]\n",
      "Epoch: 243 Training loss: [0.25390506] Validation loss: [0.2611928]\n",
      "Epoch: 244 Training loss: [0.25330746] Validation loss: [0.26052922]\n",
      "Epoch: 245 Training loss: [0.25271469] Validation loss: [0.25987074]\n",
      "Epoch: 246 Training loss: [0.25212705] Validation loss: [0.25921848]\n",
      "Epoch: 247 Training loss: [0.25154454] Validation loss: [0.25857171]\n",
      "Epoch: 248 Training loss: [0.25096709] Validation loss: [0.25793058]\n",
      "Epoch: 249 Training loss: [0.25039569] Validation loss: [0.25729489]\n",
      "Epoch: 250 Training loss: [0.24982886] Validation loss: [0.2566649]\n",
      "Epoch: 251 Training loss: [0.24926747] Validation loss: [0.25604135]\n",
      "Epoch: 252 Training loss: [0.24871086] Validation loss: [0.25542349]\n",
      "Epoch: 253 Training loss: [0.24815924] Validation loss: [0.25481096]\n",
      "Epoch: 254 Training loss: [0.24761315] Validation loss: [0.25420386]\n",
      "Epoch: 255 Training loss: [0.24707231] Validation loss: [0.25360161]\n",
      "Epoch: 256 Training loss: [0.24653602] Validation loss: [0.253003]\n",
      "Epoch: 257 Training loss: [0.24600391] Validation loss: [0.25240964]\n",
      "Epoch: 258 Training loss: [0.24547637] Validation loss: [0.25182092]\n",
      "Epoch: 259 Training loss: [0.24495389] Validation loss: [0.25123686]\n",
      "Epoch: 260 Training loss: [0.24443655] Validation loss: [0.25065815]\n",
      "Epoch: 261 Training loss: [0.24392372] Validation loss: [0.25008419]\n",
      "Epoch: 262 Training loss: [0.24341564] Validation loss: [0.24951576]\n",
      "Epoch: 263 Training loss: [0.24291214] Validation loss: [0.24895245]\n",
      "Epoch: 264 Training loss: [0.24241351] Validation loss: [0.24839449]\n",
      "Epoch: 265 Training loss: [0.2419183] Validation loss: [0.24784248]\n",
      "Epoch: 266 Training loss: [0.24142717] Validation loss: [0.24729508]\n",
      "Epoch: 267 Training loss: [0.24094044] Validation loss: [0.24675393]\n",
      "Epoch: 268 Training loss: [0.24045801] Validation loss: [0.24621715]\n",
      "Epoch: 269 Training loss: [0.23997962] Validation loss: [0.24568434]\n",
      "Epoch: 270 Training loss: [0.23950575] Validation loss: [0.24515444]\n",
      "Epoch: 271 Training loss: [0.23903571] Validation loss: [0.24462825]\n",
      "Epoch: 272 Training loss: [0.23856948] Validation loss: [0.24410728]\n",
      "Epoch: 273 Training loss: [0.23810761] Validation loss: [0.24359204]\n",
      "Epoch: 274 Training loss: [0.23765025] Validation loss: [0.24308121]\n",
      "Epoch: 275 Training loss: [0.23719558] Validation loss: [0.24257366]\n",
      "Epoch: 276 Training loss: [0.23674379] Validation loss: [0.24206939]\n",
      "Epoch: 277 Training loss: [0.23629545] Validation loss: [0.24156748]\n",
      "Epoch: 278 Training loss: [0.23585111] Validation loss: [0.24106863]\n",
      "Epoch: 279 Training loss: [0.23541094] Validation loss: [0.24057467]\n",
      "Epoch: 280 Training loss: [0.23497443] Validation loss: [0.24008574]\n",
      "Epoch: 281 Training loss: [0.23454252] Validation loss: [0.2396017]\n",
      "Epoch: 282 Training loss: [0.23411468] Validation loss: [0.23912239]\n",
      "Epoch: 283 Training loss: [0.23369052] Validation loss: [0.23864639]\n",
      "Epoch: 284 Training loss: [0.23327021] Validation loss: [0.23817308]\n",
      "Epoch: 285 Training loss: [0.23285371] Validation loss: [0.23770346]\n",
      "Epoch: 286 Training loss: [0.23244123] Validation loss: [0.23723565]\n",
      "Epoch: 287 Training loss: [0.23203278] Validation loss: [0.23677109]\n",
      "Epoch: 288 Training loss: [0.23162782] Validation loss: [0.23630761]\n",
      "Epoch: 289 Training loss: [0.23122589] Validation loss: [0.23584792]\n",
      "Epoch: 290 Training loss: [0.23082751] Validation loss: [0.23539205]\n",
      "Epoch: 291 Training loss: [0.23043261] Validation loss: [0.23493962]\n",
      "Epoch: 292 Training loss: [0.23004122] Validation loss: [0.23448987]\n",
      "Epoch: 293 Training loss: [0.22965278] Validation loss: [0.2340426]\n",
      "Epoch: 294 Training loss: [0.22926718] Validation loss: [0.23359789]\n",
      "Epoch: 295 Training loss: [0.22888577] Validation loss: [0.23315527]\n",
      "Epoch: 296 Training loss: [0.22850691] Validation loss: [0.23271492]\n",
      "Epoch: 297 Training loss: [0.22813115] Validation loss: [0.23227757]\n",
      "Epoch: 298 Training loss: [0.22775824] Validation loss: [0.23184377]\n",
      "Epoch: 299 Training loss: [0.22738828] Validation loss: [0.2314136]\n",
      "Epoch: 300 Training loss: [0.22702229] Validation loss: [0.23098718]\n",
      "Model saved in file: /tmp/earthquake_model.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbf2bcd2be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = neural_network_model(x)\n",
    "Loss =  tf.reduce_mean(tf.square(y-prediction))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(Loss)\n",
    "#optimizer = tf.train.AdagradOptimizer(0.01).minimize(Loss)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate,0.01).minimize(Loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(Loss)\n",
    "#Operation to save variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epoch = 300\n",
    "train = []\n",
    "valid = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(\" Intial Training loss:\",sess.run([Loss],feed_dict={x:trainX,y:trainY}))\n",
    "    for i in range(epoch):\n",
    "        sess.run([optimizer],feed_dict={x:trainX,y:trainY})\n",
    "        train_loss = sess.run([Loss],feed_dict={x:trainX,y:trainY})\n",
    "        valid_loss = sess.run([Loss],feed_dict={x:validX,y:validY})\n",
    "        train += [train_loss]\n",
    "        valid += [valid_loss]  \n",
    "        print(\"Epoch:\",i+1,\"Training loss:\",train_loss,\"Validation loss:\",valid_loss)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(train)),train,'r',valid,'b')\n",
    "    plt.legend(['train','valid'])\n",
    "    \n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, \"/tmp/earthquake_model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Test: [array([[ 5.89184475],\n",
      "       [ 5.2868433 ],\n",
      "       [ 5.87542343],\n",
      "       ..., \n",
      "       [ 5.87458277],\n",
      "       [ 5.92343426],\n",
      "       [ 5.77965069]], dtype=float32)]\n",
      "Test Loss: [0.24450813]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk for validation.\n",
    "    saver.restore(sess, \"/tmp/earthquake_model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    #print(\"Final validation loss:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "    print(\"Test:\",sess.run([prediction],feed_dict={x:testX}))\n",
    "    print(\"Test Loss:\",sess.run([Loss],feed_dict={x:testX,y:testY}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
